# End-to-End Speech Interaction Research

This repository collects common datasets, APIs, and papers related to the research on end-to-end speech interaction, including ASR, S2TT, emotion-aware interaction, and voice style conversion.

This repository is continuously updatingğŸ‰

<!-- If this repository brings you some inspiration, I would be very honoredğŸ˜Š -->

<!-- If you have any suggestions, feel free to contact me with: your-email@example.comğŸ“® -->

<!-- Additionally, if you could consider giving my repository a starğŸŒŸ, that would motivate me a lot! -->

## Contents

- [Datasets](#Datasets)
- [End-to-End Speech Interaction APIs](#paper)
- [Pipelines](#pipelines)
<!-- - [Popular Datasets](#available-datasets) -->

<!-- ## Datasets
<a id="Datasets"></a>
### Speech Interaction Datasets

- **Librispeech**: 1000 hours of English read speech at 16kHz. [[Link](https://www.openslr.org/12)]
- **Common Voice 15**: 7335 Hours,60 Languages[[Link](https://paperswithcode.com/dataset/common-voice)]
- **Fleurs**: 102 Languages,12 hours per language [[Link](https://huggingface.co/datasets/google/fleurs)]
- **Aishell2**: 1000 hours of clean read-speech data from iOS,Available for Mac/iOS/Android. [Link]
- **CoVoST2**: Multilingual dataset including English to multiple languages. [[Link](https://github.com/kaldi-asr/kaldi/tree/master/egs/aishell2)]
- **Meld**: Multimodal emotion dataset. MELD has more than 1400 dialogues and 13000 utterances from Friends TV series[[Link](https://affective-meld.github.io/)]
- **VocalSound**: 21,024 crowdsourced recordings of laughter, sighs, coughs, throat clearing, sneezes, and sniffs from 3,365 unique subjects[[Link](https://sls.csail.mit.edu/downloads/vocalsound/)]
- **Chat Benchmark**: Speech | Sound | Music | Mixed-Audio. [Link]
- **MOSS dataset**: Synthetic dataset[[Link](https://huggingface.co/datasets/YeungNLP/moss-003-sft-data)]
- **Wenetspeech**: 22400 hours[[Link](https://wenet.org.cn/WenetSpeech/)]
- **EMOBox**: Emotion-labeled dataset for speech. [[Link](https://github.com/emo-box/EmoBox)]
- **AirBench**: [Link]
- **SWAB**: [Link]

### In-house Test Datasets
- **Seed-TTS**: [Link] -->

## End-to-End Speech Interaction APIs
<a id="paper"></a>

### [Qwen2-Audio](https://github.com/QwenLM/Qwen2-Audio)
- Hugging Face Model: [Qwen2-Audio-7B-Instruct](https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct)

### [Mini-Omni](https://github.com/gpt-omni/mini-omni)
- Fine-tuned on Qwen2-0.5B

### [MinMo (é€šä¹‰)](https://funaudiollm.github.io/minmo/)
- Real-time interruption interaction support
- Fine-tuned on Qwen2.5-7B-Instruct

### [GLM-4-Voice](https://github.com/THUDM/GLM-4-Voice/tree/main)

### [Step-Audio-Chat (é˜¶è·ƒæ˜Ÿè¾°)](https://github.com/stepfun-ai/Step-Audio)
- Model: [Step-Audio-Chat](https://modelscope.cn/models/stepfun-ai/Step-Audio-Chat/files)

## Pipelines
<a id="pipelines"></a>

- **ASR-LLM-TTS**: Traditional pipeline combining ASR, LLM, and TTS.
- **End-to-End Pipelines**: Non-autoregressive text-to-speech models with language models using connectionist temporal loss.

<!-- ## Emotion-Aware Interaction

### LUCY
- Based on Mini-Omni's architecture, specifically fine-tuned for emotion control and function calling. -->

<!-- ## Available Datasets
<a id="available-datasets"></a>

- **Librispeech**
- **Common Voice 15**: English, Chinese, Cantonese, French
- **Fleurs**: Chinese
- **Aishell2**: Mac/iOS/Android
- **CoVoST2**: en-de | de-en | en-zh | zh-en | es-en | fr-en | it-en
- **Meld**
- **VocalSound**
- **Chat Benchmark**: Speech | Sound | Music | Mixed-Audio
- **Synthetic Datasets**: MOSS dataset
- **Wenetspeech**
- **CommonVoice**
- **EMOBox**
- **AirBench**
- **SWAB**
- **Seed-TTS**
- **Llama Questions**
- **TriviaQA**
- **AlpacaEval**
- **In-house ChitChat**
- **Alimeeting**
- **Fisher**
- **Simulation**
- **VoiceAssistant-400K**
- **VCTK**
- **Multilingual LibriSpeech**
- **Libritts**
- **Open-Orca**
- **OpenAssistant**
- **Trivia-Multichoice**
- **Trivia-Singlechoice**
- **Rlhf**
- **QAassistant** -->


# è¯­éŸ³æ•°æ®é›†æ±‡æ€»
<a id="Datasets"></a>

## 1.Librispeech[[Link](https://www.openslr.org/12)]

| **å±æ€§**     | **å†…å®¹**                                         |
|--------------|--------------------------------------------------|
| **è¯­ç§**     | English                                          |
| **è¯­éŸ³ç±»å‹** | æœ—è¯»ã€æ¼”è®²                                       |
| **å½¢å¼ç±»å‹** | éå¯¹è¯ï¼Œå•å£°é“                                   |
| **æ€»æ—¶é•¿ä¸æ•°æ®é›†åˆ’åˆ†** | æ€»æ—¶é•¿ï¼šçº¦1000å°æ—¶ï¼Œåˆ†ä¸º cleanï¼ˆè¾ƒé«˜è´¨é‡ï¼‰å’Œ otherï¼ˆåŒ…å«æ›´å¤šå™ªå£°ï¼‰ä¸¤ç±» |

---

## 2.Common Voice[[Link](https://paperswithcode.com/dataset/common-voice)]

| **å±æ€§**     | **å†…å®¹**                                         |
|--------------|--------------------------------------------------|
| **è¯­ç§**     | Englishã€Chineseã€Frenchã€Germanç­‰120ç§è¯­è¨€    |
| **è¯­éŸ³ç±»å‹** | æœ—è¯»ï¼Œå°‘é‡å¯¹è¯                                   |
| **å½¢å¼ç±»å‹** | éå¯¹è¯ï¼Œå•å£°é“ï¼ˆ16kHzï¼‰ï¼Œæå°‘éƒ¨åˆ†åŒå£°é“          |
| **æ€»æ—¶é•¿ä¸æ•°æ®é›†åˆ’åˆ†** | æ€»æ—¶é•¿ï¼šçº¦24000å°æ—¶ï¼Œè‹±è¯­å æ¯”æœ€é«˜ï¼ˆçº¦ 10,000+ å°æ—¶ï¼‰ï¼Œä¸­æ–‡çº¦ 3,000+ å°æ—¶ |

---

## 3.Fleurs[[Link](https://huggingface.co/datasets/google/fleurs)]

| **å±æ€§**     | **å†…å®¹**                                         |
|--------------|--------------------------------------------------|
| **è¯­ç§**     | Englishã€Chineseã€Frenchã€Germanç­‰102ç§è¯­è¨€    |
| **è¯­éŸ³ç±»å‹** | æ–°é—»æœ—è¯»                                         |
| **å½¢å¼ç±»å‹** | éå¯¹è¯ï¼Œå•å£°é“ï¼ˆ16kHzï¼‰                          |
| **æ€»æ—¶é•¿ä¸æ•°æ®é›†åˆ’åˆ†** | æ€»æ—¶é•¿ï¼šçº¦12000å°æ—¶ï¼Œè‹±è¯­ã€ä¸­æ–‡ã€æ³•è¯­ç­‰æ¯ç§è¯­è¨€300å°æ—¶ï¼Œå¤§éƒ¨åˆ†è¯­ç§æ•°æ®é‡ä¸¥æ ¼å‡è¡¡ |

---

## 4.Aishell2[[Link](https://github.com/kaldi-asr/kaldi/tree/master/egs/aishell2)]

| **å±æ€§**     | **å†…å®¹**                                         |
|--------------|--------------------------------------------------|
| **è¯­ç§**     | Chinese                                          |
| **è¯­éŸ³ç±»å‹** | æœ—è¯»40%ï¼Œè‡ªç„¶å¯¹è¯60%                             |
| **å½¢å¼ç±»å‹** | å¤šäººå¯¹è¯ï¼Œå¤šå£°é“ï¼Œå•äººå¯¹è¯ï¼Œå•å£°é“               |
| **æ€»æ—¶é•¿ä¸æ•°æ®é›†åˆ’åˆ†** | æ€»æ—¶é•¿ï¼šçº¦1000å°æ—¶ï¼Œé¢å‘æ™ºèƒ½å®¶å±…ä¸ç§»åŠ¨ç«¯è¯­éŸ³äº¤äº’ï¼Œè·¨è®¾å¤‡å½•åˆ¶ï¼ŒåŒ…å«æ‰‹æœºã€å¹³æ¿ã€æ™ºèƒ½éŸ³ç®±ã€iOSã€Androidã€Macç­‰ |

---

## 5.CoVoST2[[Link](https://paperswithcode.com/dataset/covost2)]

| **å±æ€§**     | **å†…å®¹**                                         |
|--------------|--------------------------------------------------|
| **è¯­ç§**     | è¦†ç›–äº†21ç§è¯­è¨€åˆ°è‹±è¯­å’Œä»è‹±è¯­åˆ°15ç§è¯­è¨€çš„æ•°æ®åº“ï¼ŒåŒ…æ‹¬æ³•è¯­ã€å¾·è¯­ã€è·å…°è¯­ã€ä¿„è¯­ã€è¥¿ç­ç‰™è¯­ã€æ„å¤§åˆ©è¯­ç­‰ |
| **è¯­éŸ³ç±»å‹** | æœ—è¯»                                             |
| **å½¢å¼ç±»å‹** | éå¯¹è¯ï¼Œå•å£°é“ï¼ˆ16kHzï¼‰                          |
| **æ€»æ—¶é•¿ä¸æ•°æ®é›†åˆ’åˆ†** | æ€»æ—¶é•¿ï¼šçº¦2880å°æ—¶ï¼Œé‡‡ç”¨ Common Voice æ•°æ®åº“ä¸­çš„è¯­éŸ³å½•éŸ³åˆ›å»º |

---

## 6.Meld[[Link](https://affective-meld.github.io/)]

| **å±æ€§**     | **å†…å®¹**                                         |
|--------------|--------------------------------------------------|
| **è¯­ç§**     | è‹±è¯­ |
| **è¯­éŸ³ç±»å‹** | ç”µå½±ä¸­çš„å¯¹è¯                                             |
| **å½¢å¼ç±»å‹** | å¯¹è¯ï¼Œå•å£°é“                          |
| **æ€»æ—¶é•¿ä¸æ•°æ®é›†åˆ’åˆ†** | æ€»æ—¶é•¿ï¼šçº¦29å°æ—¶ï¼Œæºè‡ªç”µè§†å‰§ã€Šè€å‹è®°ã€‹ï¼Œ1,400 ä¸ªå¯¹è¯å’Œ 13,000 ä¸ªå¥å­ã€‚è¿™äº›å¯¹è¯æ¶‰åŠå¤šä¸ª speakerï¼Œå¯¹è¯ä¸­çš„æ¯ä¸ªå¥å­éƒ½è¢«æ ‡è®°ä¸ºä¸ƒç§æƒ…ç»ªä¸­çš„å…¶ä¸­ä¸€ç§ï¼š æ„¤æ€’ã€åŒæ¶ã€æ‚²ä¼¤ã€å¿«ä¹ã€ä¸­ç«‹ã€æƒŠè®¶å’Œææƒ§ |

---

## 7.VocalSound[[Link](https://sls.csail.mit.edu/downloads/vocalsound/)]

| **å±æ€§**     | **å†…å®¹**                                         |
|--------------|--------------------------------------------------|
| **è¯­ç§**     | è‹±è¯­ |
| **è¯­éŸ³ç±»å‹** | éå¯¹è¯éŸ³é¢‘                                            |
| **å½¢å¼ç±»å‹** | éå¯¹è¯ï¼Œå•å£°é“                          |
| **æ€»æ—¶é•¿ä¸æ•°æ®é›†åˆ’åˆ†** | éŸ³é¢‘é•¿åº¦çš„å¹³å‡å€¼æ˜¯4.18ç§’ï¼Œæ€»æ—¶é•¿ï¼š23å°æ—¶ï¼ŒåŒ…å« 3,365 åä¸åŒå—è¯•è€…çš„ 21,024 æ¡ä¼—åŒ…å½•éŸ³ï¼ŒåŒ…æ‹¬ç¬‘å£°ã€å¹æ°”å£°ã€å’³å—½å£°ã€æ¸…å—“å­å£°ã€æ‰“å–·åšå£°å’Œå¸æ°”å£° |

---

## 8.Wenetspeech[[Link](https://wenet.org.cn/WenetSpeech/)]

| **å±æ€§**     | **å†…å®¹**                                         |
|--------------|--------------------------------------------------|
| **è¯­ç§**     | ä¸­æ–‡ |
| **è¯­éŸ³ç±»å‹** | æœ‰å£°è¯»ç‰©ï¼Œç°åœºè§£è¯´ï¼Œçºªå½•ç‰‡ï¼Œæˆå‰§ï¼Œé‡‡è®¿ï¼Œæ–°é—»ï¼Œé˜…è¯»ï¼Œè®¨è®ºï¼Œç»¼è‰º                                            |
| **å½¢å¼ç±»å‹** | å•å£°é“                          |
| **æ€»æ—¶é•¿ä¸æ•°æ®é›†åˆ’åˆ†** | æ€»æ—¶é•¿ï¼š10000+å°æ—¶ï¼Œæ•°æ®å‡æ¥è‡ª YouTube å’Œ Podcast |

---

## 9.EMOBox[[Link](https://github.com/emo-box/EmoBox)]

| **å±æ€§**     | **å†…å®¹**                                         |
|--------------|--------------------------------------------------|
| **è¯­ç§**     | æ¶µç›–14ç§ä¸åŒçš„è¯­è¨€ï¼ŒåŒ…æ‹¬12ä¸ªè‹±è¯­æ•°æ®é›†ã€3ä¸ªæ™®é€šè¯æ•°æ®é›†ï¼Œä»¥åŠå„2ä¸ªæ³•è¯­ã€å¾·è¯­å’Œæ„å¤§åˆ©è¯­æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œé˜¿å§†å“ˆæ‹‰è¯­ã€å­ŸåŠ æ‹‰è¯­ã€å¸Œè…Šè¯­ã€æ³¢æ–¯è¯­ã€æ³¢å…°è¯­ã€ä¿„è¯­ã€è¥¿ç­ç‰™è¯­ã€åœŸè€³å…¶è¯­å’Œä¹Œå°”éƒ½è¯­å„æœ‰1ä¸ªæ•°æ®é›† |
| **è¯­éŸ³ç±»å‹** | è®¿è°ˆã€å¯¹è¯ã€æ¼”è®²ã€è¾©è®ºã€æœ—è¯»                                            |
| **å½¢å¼ç±»å‹** | å•å£°é“                          |
| **æ€»æ—¶é•¿ä¸æ•°æ®é›†åˆ’åˆ†** | æ€»æ—¶é•¿ï¼š294.4å°æ—¶ |

---

## 10.AirBench[[Link](https://github.com/emo-box/EmoBox)]

| **å±æ€§**     | **å†…å®¹**                                         |
|--------------|--------------------------------------------------|
| **è¯­ç§**     | English, Chinese, Spanish, French, German, Russian, Japanese, Korean, Arabic, Persian, Indonesian, Hindi, Bengali |
| **è¯­éŸ³ç±»å‹** |                                             |
| **å½¢å¼ç±»å‹** |                           |
| **æ€»æ—¶é•¿ä¸æ•°æ®é›†åˆ’åˆ†** |  |

---
